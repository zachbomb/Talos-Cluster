# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: ollama
  namespace: ollama
spec:
  releaseName: ollama
  interval: 15m
  timeout: 10m
  chart:
    spec:
      chart: ollama
      version: 8.23.15
      sourceRef:
        kind: HelmRepository
        name: truecharts
        namespace: flux-system
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    image:
      repository: docker.io/ollama/ollama
      pullPolicy: IfNotPresent
      tag: 0.16.1@sha256:dca1224ecd799f764b21b8d74a17afdc00505ecce93e7b55530d124115b42260
    rocmImage:
      repository: docker.io/ollama/ollama
      pullPolicy: IfNotPresent
      tag: 0.16.1-rocm@sha256:dabd1c5ceb4260e48519a1235b647707cd773c146a3bc592bf32357ad54cc343
    uiImage:
      repository: ghcr.io/open-webui/open-webui
      pullPolicy: IfNotPresent
      tag: latest@sha256:f1c28d8f57ce1c759dd61351d5b90e30627cdebe6725ba45f4d02c432b7fd21b
    credentials:
      s3:
        type: s3
        url: ${S3URL}
        bucket: "${S3PREFIX}-ollama"
        accessKey: ${S3ID}
        secretKey: ${S3KEY}
        encrKey: ${S3KEY}
    ingress:
      main:
        enabled: true
        hosts:
          - host: ollama.${BASE_DOMAIN}
            paths:
              - path: /
                pathType: Prefix
        integrations:
          nginx:
            enabled: true
            ingressClassName: internal
          traefik:
            enabled: false
            middlewares:
              - name: local
                namespace: traefik
          certManager:
            enabled: true
            certificateIssuer: wethecommon-prod-cert
          homepage:
            enabled: true
            name: Ollama
            description: AI Language Model
            group: Lifestyle & Home
            widget:
              enabled: false
    service:
      main:
        enabled: true
        type: LoadBalancer
        loadBalancerIP: ${OLLAMA_IP}
        annotations:
          metallb.io/loadBalancerIPs: ${OLLAMA_IP}
        ports:
          main:
            enabled: true
            port: 10686
            targetPort: 10686
            protocol: tcp
          api:
            enabled: true
            port: 11434
            targetPort: 11434
            protocol: tcp
      api:
        enabled: false
    ollama:
      registration:
        enabled: true
        def_user_role: "pending"
      whisper:
        model: "base"
      rag:
        model_device_type: "cpu"
        model: "all-MiniLM-L6-v2"
    workload:
      main:
        podSpec:
          runtimeClassName: "nvidia"
          containers:
            main:
              imageSelector: image
              env:
                OLLAMA_BIND_ADDRESS: "0.0.0.0"
              probes:
                liveness:
                  enabled: true
                  type: http
                  path: /api/version
                  port: 11434
                readiness:
                  enabled: true
                  type: http
                  path: /api/version
                  port: 11434
                startup:
                  enabled: true
                  type: tcp
                  port: 11434
      ui:
        enabled: true
        type: Deployment
        podSpec:
          containers:
            ui:
              primary: true
              enabled: true
              imageSelector: uiImage
              probes:
                liveness:
                  enabled: true
                  type: http
                  path: /
                  port: 10686
                readiness:
                  enabled: true
                  type: http
                  path: /
                  port: 10686
                startup:
                  enabled: true
                  type: tcp
                  port: 10686
              env:
                PORT: "10686"
                OLLAMA_BASE_URL: "http://${OLLAMA_IP}:11434"
                ENABLE_SIGNUP: "true"
                DEFAULT_USER_ROLE: "pending"
                WHISPER_MODEL: "base"
                RAG_EMBEDDING_MODEL: "all-MiniLM-L6-v2"
                RAG_EMBEDDING_MODEL_DEVICE_TYPE: "cpu"
                WEBUI_SECRET_KEY:
                  secretKeyRef:
                    name: ollama-secrets
                    key: WEBUI_SECRET_KEY
    configmap:
      tcportal-open:
        enabled: true
        data:
          placeholder: "enabled"
    persistence:
      config:
        enabled: true
        mountPath: /root/.ollama
        volsync:
          - name: config
            type: restic
            credentials: s3
            src:
              enabled: true
              cacheCapacity: 10Gi
            dest:
              enabled: false
              cacheCapacity: 10Gi
      data:
        enabled: true
        mountPath: /app/backend/data
        volsync:
          - name: data
            type: restic
            credentials: s3
            src:
              enabled: true
              cacheCapacity: 5Gi
            dest:
              enabled: false
              cacheCapacity: 5Gi
    autoscaling:
      vpa:
        enabled: true
    resources:
      requests:
        memory: 512Mi
        cpu: 25m
      limits:
        memory: 8Gi
        cpu: 500m
        nvidia.com/gpu: 1
